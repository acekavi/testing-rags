# ===========================================
# RAG Q&A API Configuration
# ===========================================
# Copy this file to .env:
#   cp .env.example .env
#
# No API keys needed! Everything runs locally.
# ===========================================

# ----- Ollama Configuration -----
# Ollama runs locally in Docker - no API key needed!
OLLAMA_HOST=localhost
OLLAMA_PORT=11434

# Model to use for generation
# Recommended for CPU (no GPU):
#   - mistral      (7B)  - Best quality, ~4GB RAM, slower on CPU
#   - llama3.2     (3B)  - Fast, good quality, ~2GB RAM
#   - phi3         (3.8B) - Microsoft's efficient model
#   - gemma2:2b    (2B)  - Google's small model, very fast
LLM_MODEL_NAME=mistral

# ----- Embedding Configuration -----
# Local embedding model (runs on your machine, no API costs)
# all-MiniLM-L6-v2 is small (80MB) and fast
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2

# ----- Chunking Configuration -----
# How many characters per chunk (smaller = more precise, larger = more context)
CHUNK_SIZE=512

# Overlap between chunks (prevents cutting sentences in half)
CHUNK_OVERLAP=50

# ----- Retrieval Configuration -----
# How many chunks to retrieve per query (for baseline /ask endpoint)
TOP_K=5

# ----- Reranking Configuration -----
# Cross-encoder model for reranking (used by /ask-reranked endpoint)
# This model is slower but more accurate than bi-encoders
# Runs locally, ~90MB download on first use
RERANKER_MODEL_NAME=cross-encoder/ms-marco-MiniLM-L-6-v2

# Number of initial candidates to retrieve before reranking
# /ask-reranked retrieves RERANKER_INITIAL_K candidates, then reranks to TOP_K
# Higher = more accurate but slower (recommended: 15-30)
RERANKER_INITIAL_K=20

# ----- ChromaDB Configuration -----
# ChromaDB server URL (Docker container)
CHROMA_HOST=localhost
CHROMA_PORT=8000

# Collection name for storing document embeddings
CHROMA_COLLECTION_NAME=documents

# ----- Data Configuration -----
# Directory containing documents to ingest
DATA_DIR=./data
