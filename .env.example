# ===========================================
# RAG Q&A API Configuration
# ===========================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env
# ===========================================

# ----- LLM Configuration -----
# Your Anthropic API key (get from https://console.anthropic.com/)
ANTHROPIC_API_KEY=your-api-key-here

# Claude model to use for generation
# Options: claude-3-haiku-20240307, claude-3-sonnet-20240229, claude-3-opus-20240229
LLM_MODEL_NAME=claude-3-haiku-20240307

# ----- Embedding Configuration -----
# Local embedding model (runs on your machine, no API costs)
# all-MiniLM-L6-v2 is small (80MB) and fast
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2

# ----- Chunking Configuration -----
# How many characters per chunk (smaller = more precise, larger = more context)
CHUNK_SIZE=512

# Overlap between chunks (prevents cutting sentences in half)
CHUNK_OVERLAP=50

# ----- Retrieval Configuration -----
# How many chunks to retrieve per query
TOP_K=5

# ----- ChromaDB Configuration -----
# ChromaDB server URL (Docker container)
CHROMA_HOST=localhost
CHROMA_PORT=8000

# Collection name for storing document embeddings
CHROMA_COLLECTION_NAME=documents

# ----- Data Configuration -----
# Directory containing documents to ingest
DATA_DIR=./data
